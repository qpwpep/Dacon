# @package train
n_splits: 5
seed: 42

early_stopping_rounds: 200
log_period: 200
use_fold_ensemble: true

# Prediction threshold for F1 and submission
threshold: 0.2

# Optional: search best threshold on OOF to maximize F1
optimize_threshold:
  enable: true
  method: pr_curve   # pr_curve | grid
  grid:
    start: 0.10
    end: 0.60
    step: 0.01
  clamp_min: 0.02
  clamp_max: 0.98

# ---------------------------
# CatBoost sub-model (optional)
# ---------------------------
catboost:
  enable: true
  # CatBoost early stopping (od_wait). If omitted -> train.early_stopping_rounds is used.
  early_stopping_rounds: ${train.early_stopping_rounds}
  # Saved fold model extension (when train.save_model && use_fold_ensemble)
  model_ext: ".cbm"
  # CatBoost params
  params:
    iterations: 5000
    learning_rate: 0.05
    depth: 8
    l2_leaf_reg: 3.0
    loss_function: "Logloss"
    eval_metric: "AUC"
    random_seed: ${train.seed}
    allow_writing_files: false
    task_type: "CPU"     # "GPU" 가능(환경에 맞게)
    verbose: ${train.log_period}
    auto_class_weights: null

# ---------------------------
# TF-IDF(char ngram) + LogisticRegression (optional)
# ---------------------------
tfidf_lr:
  enable: true
  # Text columns to concatenate (raw df columns; missing cols are ignored)
  cols:
    - whyBDA
    # - what_to_gain
    # - hope_for_group
    # - desired_career_path
    - incumbents_lecture_scale_reason
    # - contest_participation
    # - onedayclass_topic
    # - expected_domain
  joiner: "\n"
  add_col_tags: true
  normalize_ws: true
  save_model: false   # save per-fold joblib when train.save_model && use_fold_ensemble

  tfidf:
    analyzer: char  # char | char_wb
    ngram_range: [2, 5]
    min_df: 2
    max_df: 0.95
    sublinear_tf: true
    max_features: 30000

  lr:
    solver: saga
    penalty: l2
    C: 4.0
    class_weight: null  # null/None | balanced
    max_iter: 6000
    n_jobs: -1

# ---------------------------
# Ensemble (LGB + CatBoost)
# ---------------------------
fold_ensemble_decision: hard_vote # hard_vote | median_proba | mean_proba
hard_vote_majority: 0.8

ensemble:
  enable: true
  weight_search:
    enable: true
    # Weights:
    # - 2 models: (LGB + CatBoost) or (LGB + TF-IDF+LR)
    # - 3 models: (LGB + CatBoost + TF-IDF+LR)
    # Search variables:
    # - w_lgb in [min_w_lgb, max_w_lgb]
    # - w_cb  in [min_w_cb,  max_w_cb]  (only when CatBoost is enabled)
    # - w_tfidf_lr is the residual: 1 - w_lgb - w_cb (clipped/normalized safely)
    min_w_lgb: 0.0
    max_w_lgb: 1.0
    min_w_cb: 0.0
    max_w_cb: 1.0
    step: 0.01
    default_w_lgb: 0.6
    default_w_cb: 0.2

# Outputs (saved inside Hydra run dir)
out_path: submit_lgbm.csv
out_proba_path: submit_lgbm_proba.csv
oof_path: oof_proba.csv

# Save LightGBM model
save_model: true
model_path: final_model.txt

# ---------------------------
# Debug / validation auditing
# ---------------------------
debug:
  misclassified:
    enable: true
    per_fold: true
    # Limit sizes to keep artifacts manageable
    max_rows_total: 5000
    max_rows_per_fold: 2000
    # Curated slices (for quick inspection)
    topk_near_threshold: 200
    topk_high_confidence: 200
    # Error slices by raw categories (which group is hard?)
    slices:
      enable: true
      max_unique: 50
      min_n: 8
      topk_per_col: 10
    # If wandb.enable=true, log small tables (near/high_conf) as W&B Tables
    log_to_wandb: true
    attach_foldfit_inputs: true
    dump_fold_preprocess_meta: true
    max_tokens_per_multilabel_col: 30
    max_nonzero_features: 80
